{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5f1ac-6950-4edc-8c06-cb35f86b5936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451b247-6261-450b-80e6-f0f5372aa197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9f29efb1-484d-4d66-937f-a630e12953ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import upkie.envs\n",
    "\n",
    "\n",
    "class PIDController:\n",
    "    def __init__(self, kp, ki, kd, dt):\n",
    "        self.kp = kp  # Proportional gain\n",
    "        self.ki = ki  # Integral gain\n",
    "        self.kd = kd  # Derivative gain\n",
    "        self.last_error = 0\n",
    "        self.integral = 0\n",
    "        self.dt = dt\n",
    "\n",
    "    def calculate_control(self, current_value):\n",
    "        error = current_value\n",
    "        self.integral += error * self.dt  # dt is the time step\n",
    "        derivative = (error - self.last_error) / self.dt\n",
    "        control_signal = (\n",
    "            np.dot(self.kp, error) + np.dot(self.ki, self.integral) + np.dot(self.kd, derivative)\n",
    "        )\n",
    "        self.last_error = error\n",
    "        return control_signal\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_error = 0\n",
    "        self.integral = 0\n",
    "\n",
    "\n",
    "class PController:\n",
    "    def __init__(self, kp, ki, kd, dt):\n",
    "        self.kp = np.array([10., 1., 0., 1.])\n",
    "\n",
    "    def calculate_control(self, current_value):\n",
    "        control_signal = (\n",
    "            np.dot(self.kp, current_value)\n",
    "        )\n",
    "        return control_signal\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def P(x):\n",
    "    kp = np.array([10., 1., 0., 1.])\n",
    "    return np.dot(kp, x)\n",
    "\n",
    "\n",
    "def push_balance_PID(env: upkie.envs.UpkieGroundVelocity, policy, force, check_msfos = False):\n",
    "    torso_force_in_world = np.zeros(3)\n",
    "    torso_force_in_world[0] = force\n",
    "    bullet_action = {\n",
    "        \"external_forces\": {\n",
    "            \"torso\": {\n",
    "                \"force\": torso_force_in_world,\n",
    "                \"local\": False,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    observation, _ = env.reset(seed = 42)\n",
    "    for step in range(4_400):\n",
    "        action = 0.0 * env.action_space.sample()\n",
    "        action[0] = policy.calculate_control(observation)\n",
    "        if step < 400 and step >= 200:\n",
    "            env.unwrapped.bullet_extra(bullet_action)  # call before env.step\n",
    "        \n",
    "        observation, _, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset(seed = 42)\n",
    "            policy.reset()\n",
    "            if check_msfos:\n",
    "                return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e00d05-cb15-46c7-b566-52b050995d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc862949-edb5-41da-8cd9-a5e73bde0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSFOS(env, policy):\n",
    "    force_0 = .1\n",
    "    force_1 = 1.\n",
    "    while push_balance(env, policy, force_1, check_msfos = True):\n",
    "        force_0 = force_1\n",
    "        force_1 *= 2\n",
    "    \n",
    "    max_ = force_1\n",
    "    min_ = force_0\n",
    "    force_1 = (max_ + min_) / 2\n",
    "    \n",
    "    while max_ - min_ > 1e-1:\n",
    "        if push_balance(env, policy, force_1, check_msfos = True):\n",
    "            min_ = force_1\n",
    "            force_1 = (force_1 + max_) / 2\n",
    "        else:\n",
    "            max_ = force_1\n",
    "            force_1 = (force_1 + min_) / 2\n",
    "    \n",
    "    return force_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1140173d-56bd-47ba-9f10-32334b4e4463",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Spine did not process request within 100.0 ms, is it stopped?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpkieGroundVelocity-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m, frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200.0\u001b[39m)\n\u001b[1;32m      3\u001b[0m policy \u001b[38;5;241m=\u001b[39m PIDController(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m20.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m.0\u001b[39m, \u001b[38;5;241m.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.0\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.00\u001b[39m]), \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m200.\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mMSFOS\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[149], line 4\u001b[0m, in \u001b[0;36mMSFOS\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m      2\u001b[0m force_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.1\u001b[39m\n\u001b[1;32m      3\u001b[0m force_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mpush_balance\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_msfos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m     force_0 \u001b[38;5;241m=\u001b[39m force_1\n\u001b[1;32m      6\u001b[0m     force_1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[146], line 12\u001b[0m, in \u001b[0;36mpush_balance\u001b[0;34m(env, policy, force, check_msfos)\u001b[0m\n\u001b[1;32m      3\u001b[0m torso_force_in_world[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m force\n\u001b[1;32m      4\u001b[0m bullet_action \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_forces\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorso\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m observation, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4_000\u001b[39m):\n\u001b[1;32m     14\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mpredict(observation, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:186\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdeprecation(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    189\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/upkie/envs/upkie_ground_velocity.py:227\u001b[0m, in \u001b[0;36mUpkieGroundVelocity.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    212\u001b[0m     seed: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    213\u001b[0m     options: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Dict]:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"!\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Resets the environment and get an initial observation.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m          Upkie this is the full observation dictionary sent by the spine.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/upkie/envs/upkie_base_env.py:188\u001b[0m, in \u001b[0;36mUpkieBaseEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reset_rate()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reset_init_state()\n\u001b[0;32m--> 188\u001b[0m spine_observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spine_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_first_observation(spine_observation)\n\u001b[1;32m    190\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_env_observation(spine_observation)\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/upkie/spine/spine_interface.py:91\u001b[0m, in \u001b[0;36mSpineInterface.start\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"!\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Reset the spine to a new configuration.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \\param[in] config Configuration dictionary.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    \\return Observation dictionary.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_spine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_dict(config)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_request(Request\u001b[38;5;241m.\u001b[39mkStart)\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/upkie/spine/spine_interface.py:146\u001b[0m, in \u001b[0;36mSpineInterface._wait_for_spine\u001b[0;34m(self, timeout_ns)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_request() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_waiting:  \u001b[38;5;66;03m# sets are fast\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Fun fact: `not in set` is 3-4x faster than `!=` on the raspi\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# perf_counter_ns clocks ~1 us on the raspi\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m perf_counter_ns() \u001b[38;5;241m>\u001b[39m stop:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpine did not process request within \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_ns\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms, is it stopped?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_request() \u001b[38;5;241m==\u001b[39m Request\u001b[38;5;241m.\u001b[39mkError:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_request(Request\u001b[38;5;241m.\u001b[39mkNone)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Spine did not process request within 100.0 ms, is it stopped?"
     ]
    }
   ],
   "source": [
    "upkie.envs.register()\n",
    "env = gym.make(\"UpkieGroundVelocity-v3\", frequency=200.0)\n",
    "policy = PIDController(np.array([20., 1., 0.1, 0.1]), np.array([.0, .0, 0, .0]), np.array([.1, 0., .1, .00]), 1 / 200.)\n",
    "print(MSFOS(env, policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d2c49af4-052e-4c49-a68e-b0cc1f64d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_balance(env: upkie.envs.UpkieServos, policy, force, check_msfos = False):\n",
    "    torso_force_in_world = np.zeros(3)\n",
    "    torso_force_in_world[0] = force\n",
    "    bullet_action = {\n",
    "        \"external_forces\": {\n",
    "            \"torso\": {\n",
    "                \"force\": torso_force_in_world,\n",
    "                \"local\": False,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    observation, _ = env.reset(seed = 42)\n",
    "    for step in range(4_000):\n",
    "        action, _ = policy.predict(observation, deterministic=True)\n",
    "        if step < 400 and step >= 200:\n",
    "            env.unwrapped.bullet_extra(bullet_action)  # call before env.step\n",
    "        \n",
    "        observation, _, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset(seed = 42)\n",
    "            if check_msfos:\n",
    "                return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "57dec33c-bf42-4e1a-9081-9a67a49784e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for log_std: copying a param with shape torch.Size([12]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([16, 460]) from checkpoint, the shape in current model is torch.Size([16, 70]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([32, 460]) from checkpoint, the shape in current model is torch.Size([32, 70]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([12, 16]) from checkpoint, the shape in current model is torch.Size([2, 16]).\n\tsize mismatch for action_net.bias: copying a param with shape torch.Size([12]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 65\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mwith gym.make(\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        env_settings.env_id,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     53\u001b[0m policy \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/andrei/Desktop/ppo_balancer/training/2024-12-06/prostemmate_1/final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Itza, toyish\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:632\u001b[0m, in \u001b[0;36mBaseAlgorithm.set_parameters\u001b[0;34m(self, load_path_or_dict, exact_match, device)\u001b[0m\n\u001b[1;32m    629\u001b[0m         attr\u001b[38;5;241m.\u001b[39mload_state_dict(params[name])  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;66;03m# Assume attr is th.nn.Module\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m         \u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_match\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m     updated_objects\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact_match \u001b[38;5;129;01mand\u001b[39;00m updated_objects \u001b[38;5;241m!=\u001b[39m objects_needing_update:\n",
      "File \u001b[0;32m~/miniconda3/envs/robotics_course/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for log_std: copying a param with shape torch.Size([12]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([16, 460]) from checkpoint, the shape in current model is torch.Size([16, 70]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([32, 460]) from checkpoint, the shape in current model is torch.Size([32, 70]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([12, 16]) from checkpoint, the shape in current model is torch.Size([2, 16]).\n\tsize mismatch for action_net.bias: copying a param with shape torch.Size([12]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/andrei/Desktop/ppo_balancer\")\n",
    "sys.path.append(\"/Users/andrei/Desktop/ppo_balancer/ppo_balancer\")\n",
    "\n",
    "import gin\n",
    "config_path = \"/Users/andrei/Desktop/ppo_balancer/ppo_balancer/policy/operative_config.gin\"\n",
    "gin.parse_config_file(config_path)\n",
    "\n",
    "from settings import EnvSettings, PPOSettings, TrainingSettings\n",
    "from envs import make_ppo_balancer_env, make_ppo_balancer_env_servos\n",
    "#from train import CustomUpkieServos, UpkieServosWrapper\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "upkie.envs.register()\n",
    "\n",
    "ppo_settings = PPOSettings()\n",
    "env_settings = EnvSettings()\n",
    "init_state = None\n",
    "\n",
    "# parent process: trainer\n",
    "agent_frequency = env_settings.agent_frequency\n",
    "\n",
    "velocity_env = upkie.envs.UpkieServos(\n",
    "            frequency=agent_frequency,\n",
    "            regulate_frequency=False,\n",
    "            spine_config=env_settings.spine_config,\n",
    "        )\n",
    "\n",
    "# Wrap the environment with the UpkieServosWrapper\n",
    "velocity_env_flat = UpkieServosWrapper(velocity_env)\n",
    "\n",
    "env = make_ppo_balancer_env_servos(velocity_env_flat, env_settings, training=True)\n",
    "\n",
    "'''\n",
    "with gym.make(\n",
    "        env_settings.env_id,\n",
    "        frequency=env_settings.agent_frequency,\n",
    "        init_state=init_state,\n",
    "        max_ground_velocity=env_settings.max_ground_velocity,\n",
    "        regulate_frequency=True,\n",
    "        spine_config=env_settings.spine_config,\n",
    "    ) as velocity_env:\n",
    "        env = make_ppo_balancer_env(\n",
    "            velocity_env,\n",
    "            env_settings,\n",
    "            training=False,\n",
    "        )\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "policy = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs={\n",
    "        \"net_arch\": {\n",
    "            \"pi\": ppo_settings.net_arch_pi,\n",
    "            \"vf\": ppo_settings.net_arch_vf,\n",
    "        },\n",
    "    },\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "policy.set_parameters(\"/Users/andrei/Desktop/ppo_balancer/training/2024-12-06/prostemmate_1/final\") # Itza, toyish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc28ee-91f1-4608-9d8d-cb1bb3e4ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MSFOS(env, policy))\n",
    "#push_balance(env, policy, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a029c8f5-a9db-439e-9474-388220a9f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "class UpkieServosWrapper(gymnasium.Wrapper):\n",
    "    \"\"\"!\n",
    "    Wrapper for the UpkieServos environment that converts actions \n",
    "    and observations to NumPy ndarrays.\n",
    "\n",
    "    This wrapper simplifies the interaction with the UpkieServos environment\n",
    "    by converting the dictionary-based actions and observations into \n",
    "    flattened NumPy ndarrays. This can be useful for compatibility with \n",
    "    algorithms that expect ndarray inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gymnasium.Env):\n",
    "        \"\"\"!\n",
    "        Initialize the wrapper.\n",
    "\n",
    "        Args:\n",
    "            env: The UpkieServos environment to wrap.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Determine the size of the flattened action and observation arrays\n",
    "        self.action_dim = 2\n",
    "        self.observation_dim = 3 + 2\n",
    "\n",
    "        # Determine the lower and upper bounds for the flattened action and observation spaces\n",
    "        action_low = np.array([0, -1,])#-1, 0, -1])\n",
    "        action_high = np.array([1., 1.])# 1, 1.01, 1])\n",
    "        observation_low = np.concatenate([\n",
    "            np.array([-1.26, -16., -28.8])\n",
    "            for i in range(1)\n",
    "        ] + [np.array([-2., -28.8])])\n",
    "        observation_high = np.concatenate([\n",
    "            np.array([1.26, 28.8])\n",
    "            for i in range(1)\n",
    "        ] + [np.array([2., 500., 28.8])])\n",
    "\n",
    "\n",
    "        # Define the new action and observation spaces\n",
    "        self.action_space = gymnasium.spaces.Box(\n",
    "            low=action_low, high=action_high, shape=(self.action_dim,), dtype=np.float64\n",
    "        )\n",
    "        self.observation_space = gymnasium.spaces.Box(\n",
    "            low=observation_low,\n",
    "            high=observation_high,\n",
    "            shape=(self.observation_dim,),\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"!\n",
    "        Reset the environment and return the observation as an ndarray.\n",
    "\n",
    "        Returns:\n",
    "            The initial observation as an ndarray.\n",
    "        \"\"\"\n",
    "        obs_dict, info = self.env.reset(seed=seed, options=options)\n",
    "        obs_dict[\"pitch\"] = info[\"spine_observation\"][\"base_orientation\"][\"pitch\"]\n",
    "        obs_dict[\"velocity\"] = info[\"spine_observation\"][\"wheel_odometry\"][\"velocity\"]\n",
    "        obs_dict[\"pitch_vel\"] = np.linalg.norm(info[\"spine_observation\"][\"imu\"][\"angular_velocity\"])\n",
    "        \n",
    "        return self._flatten_observation(obs_dict), info\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"!\n",
    "        Take a step in the environment using an ndarray action.\n",
    "\n",
    "        Args:\n",
    "            action: The action to take as an ndarray.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the next observation as an ndarray,\n",
    "            the reward, a boolean indicating if the episode is done,\n",
    "            a boolean indicating if the episode is truncated,\n",
    "            and a dictionary containing extra information.\n",
    "        \"\"\"\n",
    "        action_dict = self._unflatten_action(action)\n",
    "        obs_dict, reward, terminated, truncated, info = self.env.step(\n",
    "            action_dict\n",
    "        )\n",
    "        \n",
    "        obs_dict[\"pitch\"] = info[\"spine_observation\"][\"base_orientation\"][\"pitch\"]\n",
    "        obs_dict[\"velocity\"] = info[\"spine_observation\"][\"wheel_odometry\"][\"velocity\"]\n",
    "        obs_dict[\"pitch_vel\"] = np.linalg.norm(info[\"spine_observation\"][\"imu\"][\"angular_velocity\"])\n",
    "        reward = self.get_reward(obs_dict, action_dict, pos = info[\"spine_observation\"][\"wheel_odometry\"][\"position\"], height = info[\"spine_observation\"][\"sim\"][\"base\"][\"position\"][2])\n",
    "\n",
    "        if info[\"spine_observation\"][\"sim\"][\"base\"][\"position\"][2] < 0.2 or abs(obs_dict[\"left_knee\"][\"position\"]) > 2:\n",
    "            terminated = True\n",
    "        \n",
    "        return (\n",
    "            self._flatten_observation(obs_dict),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def flatten_action(self, action_dict: dict) -> np.ndarray:\n",
    "        \"\"\"!\n",
    "        Flatten the dictionary-based action into an ndarray.\n",
    "    \n",
    "        Args:\n",
    "            action_dict: The action dictionary.\n",
    "    \n",
    "        Returns:\n",
    "            The flattened action as an ndarray.\n",
    "        \"\"\"\n",
    "        flat_action = np.array([action_dict[\"left_knee\"][\"position\"], action_dict[\"left_wheel\"][\"velocity\"]])\n",
    "\n",
    "        return flat_action\n",
    "\n",
    "    def _flatten_observation(self, obs_dict: dict) -> np.ndarray:\n",
    "        \"\"\"!\n",
    "        Flatten the dictionary-based observation into an ndarray.\n",
    "\n",
    "        Args:\n",
    "            obs_dict: The observation dictionary.\n",
    "\n",
    "        Returns:\n",
    "            The flattened observation as an ndarray.\n",
    "        \"\"\"\n",
    "        flat_obs = []\n",
    "        for joint_obs_key in obs_dict.keys():\n",
    "            if joint_obs_key != \"pitch\" and joint_obs_key != \"velocity\" and joint_obs_key != \"pitch_vel\":\n",
    "                if joint_obs_key == \"left_knee\":    \n",
    "                    for obs_key in obs_dict[joint_obs_key].keys():\n",
    "                        obs_value = obs_dict[joint_obs_key][obs_key]\n",
    "                        if obs_key != \"temperature\" and obs_key != \"voltage\" and obs_key != \"torque\":\n",
    "                            flat_obs.append(np.array(obs_value))\n",
    "            else:\n",
    "                flat_obs.append(np.ones(1) * obs_dict[joint_obs_key])\n",
    "\n",
    "        \n",
    "        return np.concatenate(flat_obs)\n",
    "\n",
    "    def _unflatten_action(self, action_ndarray: np.ndarray) -> dict:\n",
    "        \"\"\"!\n",
    "        Unflatten the ndarray action into a dictionary-based action.\n",
    "\n",
    "        Args:\n",
    "            action_ndarray: The action as an ndarray.\n",
    "\n",
    "        Returns:\n",
    "            The unflattened action as a dictionary.\n",
    "        \"\"\"\n",
    "        action_dict = {}\n",
    "        i = 0\n",
    "        for joint_name, joint_space in self.env.action_space.spaces.items():\n",
    "            action_dict[joint_name] = {}\n",
    "            if joint_name == \"left_hip\":\n",
    "                action_dict[joint_name][\"velocity\"] = 0\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"position\"] = - action_ndarray[0] / 2\n",
    "            elif joint_name == \"left_knee\":\n",
    "                action_dict[joint_name][\"velocity\"] = 0\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"position\"] = action_ndarray[0]\n",
    "            elif joint_name == \"left_wheel\":\n",
    "                action_dict[joint_name][\"position\"] = np.nan\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"velocity\"] = action_ndarray[1] * 28.8\n",
    "            elif joint_name == \"right_hip\":\n",
    "                action_dict[joint_name][\"position\"] = action_ndarray[0] / 2\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"velocity\"] = 0\n",
    "            elif joint_name == \"right_knee\":\n",
    "                action_dict[joint_name][\"position\"] = -action_ndarray[0]\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"velocity\"] = 0\n",
    "            elif joint_name == \"right_wheel\":\n",
    "                action_dict[joint_name][\"position\"] = np.nan\n",
    "                action_dict[joint_name][\"feedforward_torque\"] = 0\n",
    "                action_dict[joint_name][\"kd_scale\"] = 1.\n",
    "                action_dict[joint_name][\"kp_scale\"] = 1.\n",
    "                action_dict[joint_name][\"velocity\"] = -action_ndarray[1] * 28.8\n",
    "        return action_dict\n",
    "    \n",
    "    # Override the method that calculates the reward.\n",
    "    def get_reward(self, observation: dict, action: dict, pos: float, height: float) -> float:\n",
    "        \"\"\"!\n",
    "        Get reward from observation and action.\n",
    "\n",
    "        \\param observation Environment observation.\n",
    "        \\param action Environment action.\n",
    "        \\return Reward.\n",
    "        \"\"\"\n",
    "        estimated_pitch = observation[\"pitch\"]\n",
    "        estimated_ground_position = pos\n",
    "        estimated_ground_velocity = observation[\"velocity\"]\n",
    "        estimated_height = height\n",
    "        estimated_angular_velocity = observation[\"pitch_vel\"]\n",
    "\n",
    "\n",
    "        tip_height = 0.58  # [m]  # This might need adjustment for the Upkie robot\n",
    "        tip_position = estimated_ground_position + tip_height * np.sin(estimated_pitch)\n",
    "        tip_velocity = estimated_ground_velocity + tip_height * estimated_angular_velocity * np.cos(estimated_pitch)\n",
    "\n",
    "        std_position = 0.05  # [m]\n",
    "        position_reward = np.exp(-((tip_position / std_position) ** 2))\n",
    "        velocity_penalty = -abs(tip_velocity)\n",
    "\n",
    "        position_weight = 1.0  # You can adjust these weights\n",
    "        velocity_weight = 0.1\n",
    "\n",
    "        ## Shaping\n",
    "        wheel_velocity_penalty = -abs(observation[\"left_wheel\"][\"velocity\"])\n",
    "        wheel_velocity_weight = 0.1\n",
    "        height_reward = estimated_height > 0.3\n",
    "        height_weight = 0.0\n",
    "\n",
    "        return 0.5 + position_weight * position_reward + velocity_weight * velocity_penalty + height_reward * height_weight + wheel_velocity_penalty * wheel_velocity_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe047152-5eeb-49b9-816e-7dd59e53450b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
